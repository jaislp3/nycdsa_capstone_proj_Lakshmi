{"cells":[{"cell_type":"code","source":["hkong = spark.table(\"hkongpercent_1_ord_currency_35acb_csv\")\ndisplay(hkong)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nX = hkong.columns[1:311]\nassembler = VectorAssembler(inputCols=X, outputCol=\"features\")\n#print (assembler)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression()\n# Import the evaluation submodule\nimport pyspark.ml.evaluation as evals\n\n# Create a BinaryClassificationEvaluator\nevaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n# Import the tuning submodule\nimport pyspark.ml.tuning as tune\nimport numpy as np\n\n# Create the parameter grid\ngrid = tune.ParamGridBuilder()\n\n# Add the hyperparameter\ngrid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\ngrid = grid.addGrid(lr.elasticNetParam, [0, 1])\n\n# Build the grid\ngrid = grid.build()\n\ncv = tune.CrossValidator(estimator=lr,\n                         estimatorParamMaps=grid,\n                         evaluator=evaluator\n                         )"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.functions import col\nscores=[]\ntarget = hkong.columns[311:]\nfor i in target:\n  #print i\n  data = hkong.select(hkong.columns[:311] + [i])\n  new_data = data.select(*[col(s).alias('label') if s ==i  else s for s in data.columns])\n  new_data  = assembler.transform(new_data)  \n  (training, test) = new_data.randomSplit([0.7, 0.3], seed = 100)  \n  models = lr.fit(training)    \n  print i     \n  test_results = models.transform(test)\n  print (evaluator.evaluate(test_results))\n  scores.append(evaluator.evaluate(test_results))\n\nprint(scores)\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["lr_score = np.mean(scores)\nlr_score"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql.functions import col\nscores=[]\ntarget = hkong.columns[311:]\nfor i in target[0:3]:\n  #print i\n  data = hkong.select(hkong.columns[:311] + [i])\n  new_data = data.select(*[col(s).alias('label') if s ==i  else s for s in data.columns])\n  new_data  = assembler.transform(new_data)  \n  (training, test) = new_data.randomSplit([0.7, 0.3], seed = 100)  \n  models = cv.fit(training)  \n  best_lr = models.bestModel\n  print i\n  print(best_lr)    \n  test_results = best_lr.transform(test)\n  print (evaluator.evaluate(test_results))\n  scores.append(evaluator.evaluate(test_results))\n\nprint(scores)\n#print training.count()\n#print test.count()\n#display(new_data)\n"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\n# Create initial LogisticRegression model\n\n# Import the evaluation submodule\nimport pyspark.ml.evaluation as evals\n\n# Create a BinaryClassificationEvaluator\nevaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n# Import the tuning submodule\nimport pyspark.ml.tuning as tune\nimport numpy as np\n\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql.functions import col\nscores=[]\ntarget = hkong.columns[311:]\nfor i in target:\n  #print i\n  data = hkong.select(hkong.columns[:311] + [i])\n  new_data = data.select(*[col(s).alias('label') if s ==i  else s for s in data.columns])\n  new_data  = assembler.transform(new_data)  \n  (training, test) = new_data.randomSplit([0.7, 0.3], seed = 100)  \n  rfc= RandomForestClassifier()\n  models = rfc.fit(training)  \n  \n  print i\n     \n  test_results = models.transform(test)\n  print (evaluator.evaluate(test_results))\n  scores.append(evaluator.evaluate(test_results))\n\nprint(scores)\n#print training.count()\n#print test.count()\n#display(new_data)\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["mean_score = np.mean(scores)\nprint (mean_score)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nrfc= RandomForestClassifier()\n# Create the parameter grid\nparamGrid = (ParamGridBuilder()\n             .addGrid(rfc.maxDepth, [2, 4, 6])\n             .addGrid(rfc.maxBins, [20, 60])\n             .addGrid(rfc.numTrees, [5, 20])\n             .build())\n\ncv = CrossValidator(estimator=rfc, estimatorParamMaps=paramGrid, evaluator=evaluator)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["scores=[]\nbest = []"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ntarget = hkong.columns[311:]\nfor i in target[28:]:\n  data = hkong.select(hkong.columns[:311] + [i])\n  new_data = data.select(*[col(s).alias('label') if s ==i  else s for s in data.columns])\n  new_data  = assembler.transform(new_data)  \n  (training, test) = new_data.randomSplit([0.7, 0.3], seed = 100)\n  rfc= RandomForestClassifier()\n  paramGrid = (ParamGridBuilder()\n             .addGrid(rfc.maxDepth, [2, 4, 6])\n             .addGrid(rfc.maxBins, [20, 60])\n             .addGrid(rfc.numTrees, [5, 20])\n             .build())\n\n  cv = CrossValidator(estimator=rfc, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n  models = cv.fit(training)  \n  best_rfc = models.bestModel\n  #print i\n  #best.append(best_rfc)  \n  test_results = best_rfc.transform(test)\n  #print (evaluator.evaluate(test_results))\n  scores.append(evaluator.evaluate(test_results))\n\n#print(best)  \nprint(scores)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["print (scores)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["rfc_score = np.mean(scores)\nprint (rfc_score)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":15}],"metadata":{"name":"hangseng_all","notebookId":1273668252982862},"nbformat":4,"nbformat_minor":0}
